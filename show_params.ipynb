{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'data': {   'batch_size': 16,\n",
      "                'color_jitter_strength': 0.0,\n",
      "                'crop_scale': [0.3, 1.0],\n",
      "                'crop_size': 224,\n",
      "                'image_folder': '.',\n",
      "                'num_workers': 10,\n",
      "                'pin_mem': True,\n",
      "                'root_path': 'src/datasets/',\n",
      "                'use_color_distortion': False,\n",
      "                'use_gaussian_blur': False,\n",
      "                'use_horizontal_flip': False},\n",
      "    'logging': {'folder': 'src/loggings', 'write_tag': 'jepa'},\n",
      "    'mask': {   'allow_overlap': False,\n",
      "                'aspect_ratio': [0.75, 1.5],\n",
      "                'enc_mask_scale': [0.85, 1.0],\n",
      "                'min_keep': 10,\n",
      "                'num_enc_masks': 1,\n",
      "                'num_pred_masks': 4,\n",
      "                'patch_size': 14,\n",
      "                'pred_mask_scale': [0.15, 0.2]},\n",
      "    'meta': {   'copy_data': False,\n",
      "                'load_checkpoint': False,\n",
      "                'model_name': 'vit_huge',\n",
      "                'pred_depth': 12,\n",
      "                'pred_emb_dim': 384,\n",
      "                'read_checkpoint': None,\n",
      "                'use_bfloat16': True},\n",
      "    'optimization': {   'ema': [0.996, 1.0],\n",
      "                        'epochs': 5,\n",
      "                        'final_lr': 1e-06,\n",
      "                        'final_weight_decay': 0.4,\n",
      "                        'ipe_scale': 1.0,\n",
      "                        'lr': 0.001,\n",
      "                        'start_lr': 0.0002,\n",
      "                        'warmup': 40,\n",
      "                        'weight_decay': 0.04}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import yaml\n",
    "import pprint\n",
    "import copy\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from src.models import modules\n",
    "from src.models.modules import text_encoder_model, x_t2i_module, vit_predictor, SelfThenCrossBlock\n",
    "\n",
    "DEVICE_0 = 'cpu'\n",
    "\n",
    "##################\n",
    "with open('configs/in1k_vith14_ep300.yaml', 'r') as y_file:\n",
    "    params = yaml.load(y_file, Loader=yaml.FullLoader)\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init models...\n",
      "crosser_total_params=38792448\n",
      "\n",
      "\n",
      "Done init models\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    SIZE: int = 224\n",
    "    PATCH_SIZE: int = params['mask']['patch_size']\n",
    "\n",
    "    V_EMBED_DIM: int = 1280\n",
    "    T_EMBED_DIM: int = 768\n",
    "    H_EMBED_DIM: int = 768\n",
    "    PRED_EMBED_DIM: int = params['meta']['pred_emb_dim']\n",
    "\n",
    "    DROP_RATE: float = 0.15\n",
    "    ATTN_DROP_RATE: float = 0.15\n",
    "    MLP_RATIO: float = 4.0\n",
    "\n",
    "    PRED_ATTN_DEPTH: int = params['meta']['pred_depth']\n",
    "    CROSS_ATTN_DEPTH: int = 4\n",
    "\n",
    "    PRED_NUM_HEADS: int = 12\n",
    "    CROSS_NUM_HEADS: int = 8\n",
    "\n",
    "MODEL_CONFIG = ModelConfig()\n",
    "\n",
    "print(f\"Init models...\")\n",
    "\n",
    "# Target T2I Module\n",
    "crosser = x_t2i_module(\n",
    "    text_embed_dim=MODEL_CONFIG.T_EMBED_DIM,\n",
    "    vision_embed_dim=MODEL_CONFIG.V_EMBED_DIM,\n",
    "    hidden_dim=MODEL_CONFIG.H_EMBED_DIM,\n",
    "    depth=MODEL_CONFIG.CROSS_ATTN_DEPTH,\n",
    "    num_heads=MODEL_CONFIG.CROSS_NUM_HEADS,\n",
    "    mlp_ratio=MODEL_CONFIG.MLP_RATIO,\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=MODEL_CONFIG.DROP_RATE,\n",
    "    attn_drop_rate=MODEL_CONFIG.ATTN_DROP_RATE,\n",
    "    cross_block=SelfThenCrossBlock\n",
    ").to(DEVICE_0)\n",
    "crosser_total_params = sum(p.numel() for p in crosser.parameters())\n",
    "print(f\"{crosser_total_params=}\")\n",
    "\n",
    "print('\\n\\nDone init models\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init models...\n",
      "crosser_total_params=57696000\n",
      "\n",
      "\n",
      "Done init models\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    SIZE: int = 224\n",
    "    PATCH_SIZE: int = params['mask']['patch_size']\n",
    "\n",
    "    V_EMBED_DIM: int = 1280\n",
    "    T_EMBED_DIM: int = 768\n",
    "    H_EMBED_DIM: int = 768\n",
    "    PRED_EMBED_DIM: int = params['meta']['pred_emb_dim']\n",
    "\n",
    "    DROP_RATE: float = 0.15\n",
    "    ATTN_DROP_RATE: float = 0.15\n",
    "    MLP_RATIO: float = 4.0\n",
    "\n",
    "    PRED_ATTN_DEPTH: int = params['meta']['pred_depth']\n",
    "    CROSS_ATTN_DEPTH: int = 6\n",
    "\n",
    "    PRED_NUM_HEADS: int = 12\n",
    "    CROSS_NUM_HEADS: int = 10\n",
    "\n",
    "MODEL_CONFIG = ModelConfig()\n",
    "\n",
    "print(f\"Init models...\")\n",
    "\n",
    "# Target T2I Module\n",
    "crosser = x_t2i_module(\n",
    "    text_embed_dim=MODEL_CONFIG.T_EMBED_DIM,\n",
    "    vision_embed_dim=MODEL_CONFIG.V_EMBED_DIM,\n",
    "    hidden_dim=MODEL_CONFIG.H_EMBED_DIM,\n",
    "    depth=MODEL_CONFIG.CROSS_ATTN_DEPTH,\n",
    "    num_heads=MODEL_CONFIG.CROSS_NUM_HEADS,\n",
    "    mlp_ratio=MODEL_CONFIG.MLP_RATIO,\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=MODEL_CONFIG.DROP_RATE,\n",
    "    attn_drop_rate=MODEL_CONFIG.ATTN_DROP_RATE,\n",
    "    cross_block=SelfThenCrossBlock\n",
    ").to(DEVICE_0)\n",
    "crosser_total_params = sum(p.numel() for p in crosser.parameters())\n",
    "print(f\"{crosser_total_params=}\")\n",
    "\n",
    "print('\\n\\nDone init models\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init models...\n",
      "crosser_total_params=131492864\n",
      "\n",
      "\n",
      "Done init models\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    SIZE: int = 224\n",
    "    PATCH_SIZE: int = params['mask']['patch_size']\n",
    "\n",
    "    V_EMBED_DIM: int = 1280\n",
    "    T_EMBED_DIM: int = 768\n",
    "    H_EMBED_DIM: int = 1024\n",
    "    PRED_EMBED_DIM: int = params['meta']['pred_emb_dim']\n",
    "\n",
    "    DROP_RATE: float = 0.15\n",
    "    ATTN_DROP_RATE: float = 0.15\n",
    "    MLP_RATIO: float = 4.0\n",
    "\n",
    "    PRED_ATTN_DEPTH: int = params['meta']['pred_depth']\n",
    "    CROSS_ATTN_DEPTH: int = 8\n",
    "\n",
    "    PRED_NUM_HEADS: int = 12\n",
    "    CROSS_NUM_HEADS: int = 12\n",
    "\n",
    "MODEL_CONFIG = ModelConfig()\n",
    "\n",
    "print(f\"Init models...\")\n",
    "\n",
    "# Target T2I Module\n",
    "crosser = x_t2i_module(\n",
    "    text_embed_dim=MODEL_CONFIG.T_EMBED_DIM,\n",
    "    vision_embed_dim=MODEL_CONFIG.V_EMBED_DIM,\n",
    "    hidden_dim=MODEL_CONFIG.H_EMBED_DIM,\n",
    "    depth=MODEL_CONFIG.CROSS_ATTN_DEPTH,\n",
    "    num_heads=MODEL_CONFIG.CROSS_NUM_HEADS,\n",
    "    mlp_ratio=MODEL_CONFIG.MLP_RATIO,\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=MODEL_CONFIG.DROP_RATE,\n",
    "    attn_drop_rate=MODEL_CONFIG.ATTN_DROP_RATE,\n",
    "    cross_block=SelfThenCrossBlock\n",
    ").to(DEVICE_0)\n",
    "crosser_total_params = sum(p.numel() for p in crosser.parameters())\n",
    "print(f\"{crosser_total_params=}\")\n",
    "\n",
    "print('\\n\\nDone init models\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init models...\n",
      "crosser_total_params=29336832\n",
      "\n",
      "\n",
      "Done init models\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    SIZE: int = 448\n",
    "    PATCH_SIZE: int = 16\n",
    "\n",
    "    V_EMBED_DIM: int = 1280\n",
    "    T_EMBED_DIM: int = 768\n",
    "    H_EMBED_DIM: int = 768\n",
    "    PRED_EMBED_DIM: int = params['meta']['pred_emb_dim']\n",
    "\n",
    "    DROP_RATE: float = 0.15\n",
    "    ATTN_DROP_RATE: float = 0.15\n",
    "    MLP_RATIO: float = 4.0\n",
    "\n",
    "    PRED_ATTN_DEPTH: int = params['meta']['pred_depth']\n",
    "    CROSS_ATTN_DEPTH: int = 4\n",
    "\n",
    "    PRED_NUM_HEADS: int = 12\n",
    "    CROSS_NUM_HEADS: int = 8\n",
    "\n",
    "MODEL_CONFIG = ModelConfig()\n",
    "\n",
    "print(f\"Init models...\")\n",
    "\n",
    "# Target T2I Module\n",
    "crosser = x_t2i_module(\n",
    "    text_embed_dim=MODEL_CONFIG.T_EMBED_DIM,\n",
    "    vision_embed_dim=MODEL_CONFIG.V_EMBED_DIM,\n",
    "    hidden_dim=MODEL_CONFIG.H_EMBED_DIM,\n",
    "    depth=MODEL_CONFIG.CROSS_ATTN_DEPTH,\n",
    "    num_heads=MODEL_CONFIG.CROSS_NUM_HEADS,\n",
    "    mlp_ratio=MODEL_CONFIG.MLP_RATIO,\n",
    "    qkv_bias=True,\n",
    "    qk_scale=None,\n",
    "    drop_rate=MODEL_CONFIG.DROP_RATE,\n",
    "    attn_drop_rate=MODEL_CONFIG.ATTN_DROP_RATE,\n",
    ").to(DEVICE_0)\n",
    "crosser_total_params = sum(p.numel() for p in crosser.parameters())\n",
    "print(f\"{crosser_total_params=}\")\n",
    "\n",
    "print('\\n\\nDone init models\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "checkpoint = torch.load(\"trains/VQA-1732161891/epoch-30.pt\", map_location='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['crosser', 'mlp_head', 'opt', 'scaler', 'epoch', 'loss', 'val_metrics'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_proj.weight\n",
      "vision_proj.bias\n",
      "vision_norm.weight\n",
      "vision_norm.bias\n",
      "blocks.0.norm1.weight\n",
      "blocks.0.norm1.bias\n",
      "blocks.0.cross_attn.query.weight\n",
      "blocks.0.cross_attn.query.bias\n",
      "blocks.0.cross_attn.key.weight\n",
      "blocks.0.cross_attn.key.bias\n",
      "blocks.0.cross_attn.value.weight\n",
      "blocks.0.cross_attn.value.bias\n",
      "blocks.0.cross_attn.proj.weight\n",
      "blocks.0.cross_attn.proj.bias\n",
      "blocks.0.norm2.weight\n",
      "blocks.0.norm2.bias\n",
      "blocks.0.mlp.fc1.weight\n",
      "blocks.0.mlp.fc1.bias\n",
      "blocks.0.mlp.fc2.weight\n",
      "blocks.0.mlp.fc2.bias\n",
      "blocks.1.norm1.weight\n",
      "blocks.1.norm1.bias\n",
      "blocks.1.cross_attn.query.weight\n",
      "blocks.1.cross_attn.query.bias\n",
      "blocks.1.cross_attn.key.weight\n",
      "blocks.1.cross_attn.key.bias\n",
      "blocks.1.cross_attn.value.weight\n",
      "blocks.1.cross_attn.value.bias\n",
      "blocks.1.cross_attn.proj.weight\n",
      "blocks.1.cross_attn.proj.bias\n",
      "blocks.1.norm2.weight\n",
      "blocks.1.norm2.bias\n",
      "blocks.1.mlp.fc1.weight\n",
      "blocks.1.mlp.fc1.bias\n",
      "blocks.1.mlp.fc2.weight\n",
      "blocks.1.mlp.fc2.bias\n",
      "blocks.2.norm1.weight\n",
      "blocks.2.norm1.bias\n",
      "blocks.2.cross_attn.query.weight\n",
      "blocks.2.cross_attn.query.bias\n",
      "blocks.2.cross_attn.key.weight\n",
      "blocks.2.cross_attn.key.bias\n",
      "blocks.2.cross_attn.value.weight\n",
      "blocks.2.cross_attn.value.bias\n",
      "blocks.2.cross_attn.proj.weight\n",
      "blocks.2.cross_attn.proj.bias\n",
      "blocks.2.norm2.weight\n",
      "blocks.2.norm2.bias\n",
      "blocks.2.mlp.fc1.weight\n",
      "blocks.2.mlp.fc1.bias\n",
      "blocks.2.mlp.fc2.weight\n",
      "blocks.2.mlp.fc2.bias\n",
      "blocks.3.norm1.weight\n",
      "blocks.3.norm1.bias\n",
      "blocks.3.cross_attn.query.weight\n",
      "blocks.3.cross_attn.query.bias\n",
      "blocks.3.cross_attn.key.weight\n",
      "blocks.3.cross_attn.key.bias\n",
      "blocks.3.cross_attn.value.weight\n",
      "blocks.3.cross_attn.value.bias\n",
      "blocks.3.cross_attn.proj.weight\n",
      "blocks.3.cross_attn.proj.bias\n",
      "blocks.3.norm2.weight\n",
      "blocks.3.norm2.bias\n",
      "blocks.3.mlp.fc1.weight\n",
      "blocks.3.mlp.fc1.bias\n",
      "blocks.3.mlp.fc2.weight\n",
      "blocks.3.mlp.fc2.bias\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n"
     ]
    }
   ],
   "source": [
    "for name in checkpoint['crosser'].keys():\n",
    "    print(name)\n",
    "for name in checkpoint['mlp_head'].keys():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
